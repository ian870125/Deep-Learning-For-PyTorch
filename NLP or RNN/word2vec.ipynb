{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4347de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np \n",
    "import urllib.request\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import sklearn\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea2a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "CONTEXT_SIZE = 3\n",
    "EMBEDDING_DIM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a6aad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(word_id):\n",
    "    for key, val in word_to_ix.items():\n",
    "        if (val == word_id):\n",
    "            return key\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc215398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(file_path, remove_stopwords = False):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if file_path.lower().startswith('http'):\n",
    "        data = urllub.request.urlopen(file_path)\n",
    "        data = data.read().decode('utf8')\n",
    "    else:\n",
    "        data = open(file_path, encoding = 'utf8').read()\n",
    "    tokenized_data = word_tokenize(data)\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "    else:\n",
    "        stop_words = set([])\n",
    "        \n",
    "    stop_words.update(['.',',',':',';','(',')','#','--','...','\"'])\n",
    "    cleaned_words = [ i for i in tokenized_data if i not in stop_words ]\n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "520f562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = read_data('./nlp_data/word2vec_test.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abc505a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Empathy', 'for', 'the'], 'poor') (['for', 'the', 'poor'], 'may')\n"
     ]
    }
   ],
   "source": [
    "ngrams = []\n",
    "for i in range(len(test_sentence) - CONTEXT_SIZE):\n",
    "    tup = [test_sentence[j] for j in np.arange(i, i + CONTEXT_SIZE)]\n",
    "    ngrams.append((tup,test_sentence[i + CONTEXT_SIZE]))\n",
    "print(ngrams[0], ngrams[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91f44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "單字個數:  192\n"
     ]
    }
   ],
   "source": [
    "vocab = set(test_sentence)\n",
    "print('單字個數: ', len(vocab))\n",
    "\n",
    "word_to_ix = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2894d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(CBOWModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out1 = F.relu(self.linear1(embeds))\n",
    "        out2 = self.linear2(out1)\n",
    "        log_probs = F.log_softmax(out2, dim = 1)\n",
    "        return log_probs\n",
    "    \n",
    "    def predict(self, input):\n",
    "        context_idxs = torch.LongTensor([word_to_ix[w] for w in input])\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending = True)\n",
    "        res_val = res_val[0][:3]\n",
    "        res_ind = res_ind[0][:3]\n",
    "        for arg in zip(res_val, res_ind):\n",
    "            print([(key, val, arg[0]) for key, val in word_to_ix.items() if val == arg[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "742120a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = CBOWModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "for epoch in range(400):\n",
    "    total_loss = 0\n",
    "    for context, target in ngrams:\n",
    "        context_idxs = torch.LongTensor([word_to_ix[w] for w in context])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.LongTensor([word_to_ix[target]]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7119a2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('afflictions', 127, tensor(-0.0418, grad_fn=<UnbindBackward0>))]\n",
      "[('health', 168, tensor(-3.9975, grad_fn=<UnbindBackward0>))]\n",
      "[('book', 82, tensor(-5.4046, grad_fn=<UnbindBackward0>))]\n"
     ]
    }
   ],
   "source": [
    "model.predict(['of','all','human'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b0645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Empathy', ['for', 'the', 'poor']) ('for', ['the', 'poor', 'may'])\n"
     ]
    }
   ],
   "source": [
    "ngrams = []\n",
    "for i in range(len(test_sentence) - CONTEXT_SIZE):\n",
    "    tup = [test_sentence[j] for j in np.arange(i + 1, i + CONTEXT_SIZE + 1)]\n",
    "    ngrams.append((test_sentence[i], tup))\n",
    "print(ngrams[0], ngrams[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36cb68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(SkipgramModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, context_size * vocab_size)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out1 = F.relu(self.linear1(embeds))\n",
    "        out2 = self.linear2(out1)\n",
    "        log_probs = F.log_softmax(out2, dim = 1).view(CONTEXT_SIZE, -1)\n",
    "        return log_probs\n",
    "    \n",
    "    def predict(self, input):\n",
    "        context_idxs = torch.LongTensor([word_to_ix[input]])\n",
    "        res = self.forward(context_idxs)\n",
    "        res_arg = torch.argmax(res)\n",
    "        res_val, res_ind = res.sort(descending = True)\n",
    "        indices = [res_ind[i][0] for i in np.arange(0,3)]\n",
    "        for arg in indices:\n",
    "            print([(key, val) for key, val in word_to_ix.items() if val == arg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7f45f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = SkipgramModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr = .001)\n",
    "for epoch in range(550):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for context, target in ngrams:\n",
    "        context_idxs = torch.LongTensor([word_to_ix[context]])\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        target_list = torch.LongTensor([word_to_ix[w] for w in target])\n",
    "        loss = loss_function(log_probs, target_list)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f396a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', 89)]\n",
      "[('physically', 68)]\n",
      "[('incapacitating', 132)]\n"
     ]
    }
   ],
   "source": [
    "model.predict('psychologically')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b99c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f4548a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=['\\n', 'or', 'are', 'they', 'i', 'some', 'by', '—', \n",
    "            'even', 'the', 'to', 'a', 'and', 'of', 'in', 'on', 'for', \n",
    "            'that', 'with', 'is', 'as', 'could', 'its', 'this', 'other',\n",
    "            'an', 'have', 'more', 'at','don’t', 'can', 'only', 'most']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7008b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a64ac3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freqs = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16deec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('were', 2), ('not', 2), ('strong', 2), ('has', 2), (\"yankees'\", 1), ('reinforcements', 1), ('offseason', 1), ('past.', 1), ('did', 1), ('sign', 1), ('four', 1), ('major', 1), ('free-market', 1), ('shortstops', 1), ('when', 1), ('needed', 1), ('shortstops.', 1), ('instead,', 1), ('chose', 1), ('welcome', 1)]\n"
     ]
    }
   ],
   "source": [
    "word_freqs = collections.Counter()\n",
    "with open('./NLP_data/news.txt','r+', encoding='UTF-8') as f:\n",
    "    for line in f:\n",
    "        # 轉小寫、分詞\n",
    "        words = line.lower().split(' ')\n",
    "        # 統計字詞出現次數\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        for word in words:\n",
    "            if not (word in stop_words):\n",
    "                word_freqs[word] += 1\n",
    "                \n",
    "print(word_freqs.most_common(20))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c446ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adbb4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cc47270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary： ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word = vectorizer.get_feature_names()\n",
    "print (\"Vocabulary：\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f44de9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW=\n",
      " [[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print (\"BOW=\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "052a1921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF = \n",
      " [[0.     0.4388 0.542  0.4388 0.     0.     0.3587 0.     0.4388]\n",
      " [0.     0.2723 0.     0.2723 0.     0.8532 0.2226 0.     0.2723]\n",
      " [0.5528 0.     0.     0.     0.5528 0.     0.2885 0.5528 0.    ]\n",
      " [0.     0.4388 0.542  0.4388 0.     0.     0.3587 0.     0.4388]]\n"
     ]
    }
   ],
   "source": [
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(X)\n",
    "print('TF-IDF = \\n', np.around(tfidf.toarray(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd83aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.1034849000930086\n",
      "  (0, 1)\t0.43830038447620107\n",
      "  (0, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print (cosine_similarity(tfidf[-1], tfidf[:-1], dense_output=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3afe10d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e0003ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "395906e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c97cb1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is a great day.',\n",
       " 'It is even better than yesterday.',\n",
       " 'And yesterday was the best day ever.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52575366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today',\n",
       " 'is',\n",
       " 'a',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'even',\n",
       " 'better',\n",
       " 'than',\n",
       " 'yesterday',\n",
       " '.',\n",
       " 'And',\n",
       " 'yesterday',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'day',\n",
       " 'ever',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf1a7000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "' '.join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b728c07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My system keep crashing his crashed yesterday, ours crash daily'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'My system keeps crashing his crashed yesterday, ours crashes daily'\n",
    "lem = nltk.WordNetLemmatizer()\n",
    "' '.join([lem.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96af921d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "標點符號:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print('標點符號: ', string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ea10021",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Today is a great day. It is even better than yesterday.\" + \\\n",
    "     \" And yesterday was the best day ever.\"\n",
    "\n",
    "stopword_list = set(nltk.corpus.stopwords.words('english') + list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2b93fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, is_lower_case = False):\n",
    "    if is_lower_case:\n",
    "        text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text, filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad6a7f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today great day It even better yesterday And yesterday best day ever'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_text, filtered_tokens = remove_stopwords(text) \n",
    "filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5890abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('strong', 2), ('shortstops', 2), ('yankees', 1), ('reinforcements', 1), ('offseason', 1), ('past', 1), ('sign', 1), ('four', 1), ('major', 1), ('free-market', 1), ('needed', 1), ('instead', 1), ('chose', 1), ('welcome', 1), ('defense-oriented', 1), ('isiah', 1), ('kiner-falefa', 1), ('trades', 1), ('move', 1), ('aroused', 1)]\n"
     ]
    }
   ],
   "source": [
    "with open('./NLP_data/news.txt', 'r+', encoding = 'UTF-8') as f:\n",
    "    text = f.read()\n",
    "filtered_text, filtered_tokens = remove_stopwords(text, True)\n",
    "\n",
    "word_freqs = collections.Counter()\n",
    "for word in filtered_tokens:\n",
    "    word_freqs[word] += 1\n",
    "print(word_freqs.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3b3499eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = nltk.WordNetLemmatizer()\n",
    "def remove_stopwords_regex(text, is_lower_case = False):\n",
    "    if is_lower_case:\n",
    "        text = text.lower()\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [lem.lemmatize(token.strip()) for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text, filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "229b9b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('strong', 2), ('shortstop', 2), ('ha', 2), ('yankee', 1), ('reinforcement', 1), ('offseason', 1), ('past', 1), ('sign', 1), ('four', 1), ('major', 1), ('free', 1), ('market', 1), ('needed', 1), ('instead', 1), ('chose', 1), ('welcome', 1), ('defense', 1), ('oriented', 1), ('isiah', 1), ('kiner', 1)]\n"
     ]
    }
   ],
   "source": [
    "filtered_text, filtered_tokens = remove_stopwords_regex(text, True) \n",
    "word_freqs = collections.Counter()\n",
    "for word in filtered_tokens:\n",
    "    word_freqs[word] += 1\n",
    "print(word_freqs.most_common(20))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "21f44ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'korean'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem.lemmatize('korean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "954729d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('love.n.01'),\n",
       " Synset('love.n.02'),\n",
       " Synset('beloved.n.01'),\n",
       " Synset('love.n.04'),\n",
       " Synset('love.n.05'),\n",
       " Synset('sexual_love.n.02'),\n",
       " Synset('love.v.01'),\n",
       " Synset('love.v.02'),\n",
       " Synset('love.v.03'),\n",
       " Synset('sleep_together.v.01')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms = nltk.corpus.wordnet.synsets('love')\n",
    "synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "553a0875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a strong positive emotion of regard and affection'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2696ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['his love for his work', 'children need a lot of love']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8087a525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma('ugly.a.01.ugly')\n",
      "Lemma('surly.s.01.surly')\n",
      "Lemma('surly.s.01.ugly')\n",
      "Lemma('despicable.s.01.despicable')\n",
      "Lemma('despicable.s.01.ugly')\n",
      "Lemma('despicable.s.01.vile')\n",
      "Lemma('despicable.s.01.slimy')\n",
      "Lemma('despicable.s.01.unworthy')\n",
      "Lemma('despicable.s.01.worthless')\n",
      "Lemma('despicable.s.01.wretched')\n",
      "Lemma('atrocious.s.03.atrocious')\n",
      "Lemma('atrocious.s.03.frightful')\n",
      "Lemma('atrocious.s.03.horrifying')\n",
      "Lemma('atrocious.s.03.horrible')\n",
      "Lemma('atrocious.s.03.ugly')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['beautiful']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonyms = []\n",
    "for syn in nltk.corpus.wordnet.synsets('ugly'):\n",
    "    for l in syn.lemmas():\n",
    "        print(l)\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9681a708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('a', 'DT'), ('human', 'JJ'), ('being', 'VBG'), (',', ','), ('capable', 'JJ'), ('of', 'IN'), ('doing', 'VBG'), ('terrible', 'JJ'), ('things', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "text='I am a human being, capable of doing terrible things'\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "628cc494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am a human being, capable of doing terrible things']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a72a9aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
